import random
configfile: 'paths.yaml'

paths = config['paths']
species = list(config['species'].keys())
focal_species = [sp for sp, values in config['species'].items()
                    if 'focal' in values and values['focal'] is True]

if len(focal_species) != 1:
    raise ValueError(f'Must specify exactly one focal species, found {len(focal_species)}')
focal_species = focal_species[0]

other_species = [sp for sp in species if sp != focal_species]

wildcard_constraints:
    analysis_type=".*"

workdir: config['workdir']

def all_input(wildcards):
    result = expand(
                paths['absense_dir'] + '/{files}',
                files=[
                    '99PI_high_prediction.tsv',
                    '99PI_lower_prediction.tsv',
                    'failure_probabilities.tsv',
                    'parameters.tsv',
                    'predicted_bitscores.tsv',
                    'run_info.txt',
                    ],
                analysis_type='',
                )
    if config['validate_genes'] != 0:
        result.extend(
                expand(paths['absense_dir'] + '/{files}',
                    files=[
                    'run_info.txt',
                    'validation.tsv',
                    ],
                    analysis_type=paths['validation_dir'])
                )
    return result

rule all:
    input: all_input

for specie, values in config['species'].items():
    if 'file' in values:
        rule:
            """Link species local file to expected location."""
            name: f'link_{specie}'
            input: values['file']
            output: expand(paths['faa'], species=specie)
            shell:
                'ln -rs {input} {output}'

    elif 'accession' in values:
        rule:
            """Download faa file with datasets."""
            name: f'datasets_{specie}'
            output: expand(paths['faa'], species=specie)
            resources: wget_instance=1
            params: accession=values['accession']
            conda: 'envs/datasets.yaml'
            shell:
                'datasets download genome '
                    'accession {params.accession} '
                    '--exclude-genomic-cds '
                    '--exclude-gff3 '
                    '--exclude-rna '
                    '--exclude-seq '
                    f'--filename {specie}.zip '
                    '--no-progressbar '
                    '--reference '
                    '--annotated\n'
                'unzip -np '
                    f'{specie}.zip '
                    "'*.faa' "
                    '> {output}\n'
                f'rm {specie}.zip '

    elif 'url' in values:
        rule:
            """Download faa file, assumed gzipped."""
            name: f'download_{specie}'
            output: expand(paths['faa'], species=specie)
            resources: wget_instance=1
            params: url=values['url']
            shell:
                'wget --quiet '
                    '-O - '
                    '{params.url} '
                '| zcat -fq > {output} '

    else:
        raise ValueError(f'No access method found for {specie}, '
                'must specify one of [file, accession, url]')

checkpoint download_busco:
    """Get busco resources, checkpoint to find id's after download."""
    output:
        directory(paths['busco'])
    resources: wget_instance=1
    params:
        url=config['urls']['busco'],
        parent=Path(paths['busco']).parent
    shell:
        'wget --quiet '
            '-O {output}.tar.gz '
            '{params.url} \n'
        'tar -xzf {output}.tar.gz -C {params.parent} \n'
        'rm {output}.tar.gz'

rule download_all:
    input:
        paths['busco'],
        expand(paths['faa'], species=config['species'].keys()),

rule hmmer_busco:
    """Perform hmmsearch for each id.

    input:
        hmm: the hmm resource
        faa: the faa files for all species

    output:
        a text file with the top hit for each species
    """
    input:
        hmm=paths['busco_hmm'],
        faa=expand(paths['faa'], species=species),

    output:
        paths['hmm_matches']

    conda: 'envs/hmmer.yaml'

    resources:
        mem_mb=4_000,
        time=60,

    shell:
        'for input in {input.faa} ; do '
            'hmmsearch {input.hmm} $input | sed -n 15p >> {output} ; '
        'done'

def get_matches(wildcards):
    checkpoints.download_busco.get()
    ids = glob_wildcards(paths['busco_hmm']).id
    return expand(paths['hmm_matches'], id=ids)

checkpoint make_busco_fastas:
    """Generate faa of best match for each hmm id.

    input:
        matches: all hmmsearch combined output files for all ids
        faa: all faa files for all species
        focal_db: blast db for focal species
        other_dbs: blast dbs for other species

    output:
        directory with files where a gene is found for all species
        with an evalue lower than specified in config.

    blast is required in this step, which means the execution has to be shell
    because this also filters, it needs to be a checkpoint and it's best to
    limit the number of checkpoints chained together. Forgive the bashing
    """
    input:
        matches=get_matches,
        faa=expand(paths['faa'], species=species),
        focal_db=expand(paths['blast_db'], species=focal_species),
        other_dbs=expand(paths['blast_db'], species=other_species)
    output:
        directory(Path(paths['busco_faa']).parent)
    conda: 'envs/blast.yaml'

    params:
        num_species=len(species),
        check_match=r'''{
            if (NF < 9 || $1 > E_VAL){genes="ERROR"; exit}
            genes = genes $9 ","
        }
        END{print genes}''',
        extract_genes=r'''BEGIN{
            split(genes,gene_names,",");
            RS=">";
        }
        FNR==1 {count++}
        $0 ~ "^" gene_names[count] {printf ">" $0; nextfile}''',
        first_genes=r'''BEGIN{ RS=">"; }
        FNR==2 {printf ">" $0; nextfile}''',

    shell:
        'mkdir -p {output}\n'
        'for infile in {input.matches} ; do\n'
            # check if the input file has a high enough e value
            "genes=$(awk -v E_VAL={config[hmmer_e_val]} '{params.check_match}' $infile)\n"
            'if [[ "$genes" == "ERROR" ]]; then\n'
                'echo $infile failed e value check\n'
                'continue\n '
            'fi\n'
            # get basename
            'inf=${{infile##*/}}\n'
            # generate fasta file
            'outfile={output}/${{inf%.*}}.faa\n'
            "awk -v genes=$genes '{params.extract_genes}' {input.faa} > $outfile \n"
            # run blast against focal db to check close matches
            'matches=$(blastp '
                '-query $outfile '
                '-db {input.focal_db} '
                '-outfmt "6 sacc" '
                '-max_target_seqs 1 '
                '2>/dev/null )\n'
            # check each gene matches to the same target once
            'num_matches=$(echo $matches | wc -w) \n'
            'uniq_matches=$(echo $matches | tr " " "\\n" | uniq | wc -l) \n'
            'if [[ $num_matches != {params.num_species} || $uniq_matches != 1 ]] ; then\n'
                'echo $infile failed unique matches to target\n'
                'rm $outfile\n'
                'continue\n'
            'fi\n'
        'done\n'
        'combined={output}/combined_query.fa\n'
        "awk '{params.first_genes}' {output}/*.faa > $combined\n"
        'for db in {input.other_dbs}; do\n'
            # run blast against the other databases
            'blastp '
                '-query $combined '
                '-db $db '
                '-outfmt "6 qacc sacc" '
                '-max_target_seqs 1 '
                '2>/dev/null '
            # for each line
            '| while read line ; do \n'
                # get focal and queried species as array
                'line=($line)\n'
                # test the queried species is in the same file as target
                # || : catches the error return code if the file isn't found
                'query=$(grep -m1 -Fl ${{line[0]}} {output}/*.faa) || :\n'
                # skip if file can't be found, happens if it was previously removed
                'if [[ "$query" == "" ]]; then continue; fi \n'
                'if ! grep -qF ${{line[1]}} $query ; then\n'
                    # else, delete file
                    'echo $query failed reciprical best match\n'
                    'rm $query\n'
                'fi\n'
            'done\n'
        'done\n'
        # clean up combined file
        'rm $combined\n'

rule muscle:
    """Run muscle alignment.

    Because the output can be shuffled, need to prepend the species name
    to each gene first.  Deconvolved in the merging step.
    """
    input:
        paths['busco_faa']
    output:
        paths['muscle_alignment']
    conda: 'envs/muscle.yaml'
    params:
        add_species=r'''BEGIN{
            split(species,species_names,",");
            RS=">";
        }
        NR>1{print ">" species_names[NR-1] "_" $0}''',
        species=','.join(species)

    shell:
        'muscle '
            "-align <(awk -v species={params.species} '{params.add_species}' {input}) "
            '-output {output} '
            '2>/dev/null '

def merge_afa_input(wildcards):
    checkpoints.make_busco_fastas.get()
    all_ids = glob_wildcards(paths['busco_faa']).id
    # for first file, use all genes
    ids = all_ids
    if wildcards.seed != 1:
        # for other replicates, randomly draw with replacement
        random.seed(wildcards.seed)
        ids = random.choices( all_ids, k=len(all_ids))
    return expand(paths['muscle_alignment'], id=ids)

rule merge_afa:
    """Produce a merged afa file with format expected by protdist.

    As usual the first line of the file gives the number of species and the
    number of sites.

    Next come the species data. Each sequence starts on a new line, has a
    ten-character species name that must be blank-filled to be of that length,
    followed immediately by the species data in the one-letter code.
    """
    input: merge_afa_input
    output: temp(paths['merged_afa'])
    run:
        merged = {specie: [] for specie in species}
        specie = species[0]  # will be overridden
        for infile in input:
            for line in open(infile):
                if line.startswith('>'):
                    specie = line[1:6]
                    continue
                merged[specie].append(line.strip())

        for specie in merged:
            merged[specie] = ''.join(merged[specie])

        with open(output[0], 'w') as outfile:
            outfile.write(f'{len(species)} {len(merged[specie])}\n')
            for specie, sequence in merged.items():
                if len(specie) > 10:
                    specie = specie[:10]
                outfile.write(f'{specie:10}{sequence}\n')

rule protdist:
    input: ancient(paths['merged_afa'])
    output: temp(paths['protdist'])
    conda: 'envs/phylip.yaml'
    params:
        path=lambda wildcards, input: Path(input[0]).parent
    shell:
        'cd {params.path}\n'
        'echo Y | protdist >/dev/null \n'  # expected interactive

rule merge_protdists:
    input:
        expand(paths['protdist'], seed=range(1, config['protdist_replicates']+1))
    output:
        paths['distances']

    run:
        distances = {}
        for infile in input:
            for line in open(infile):
                if line.startswith(' '):
                    continue
                species, dist, *_ = line.split()
                if species not in distances:
                    distances[species] = []
                distances[species].append(dist)

        with open(output[0], 'w') as outfile:
            for species, dists in distances.items():
                outfile.write(f'{species}\t')
                outfile.write('\t'.join(dists))
                outfile.write('\n')

rule make_blast_db:
    input: paths['faa']
    output:
        alias=paths['blast_db'],
        implicit=multiext(paths['blast_db'],
                          '.pin', '.phr', '.psq',
                          '.pdb', '.pot', '.pto', '.ptf')
    conda: 'envs/blast.yaml'
    shell:
        'makeblastdb '
            '-dbtype prot '
            '-in {input} '
            '-out {output.alias} '
            '> /dev/null \n'
        'ln -rs {input} {output.alias}'

rule make_validate_targets:
    input:
        faa=expand(paths['faa'], species=focal_species)
    output: paths['validate_query_genes']
    params:
        # print only lines with >, remove > and everything after first space
        sed_script=r"'/^>/{s/^>// ; s/ .*// ; p}'",
        genes=lambda wildcards: '' if config['validate_genes'] == -1 else f'-n {config["validate_genes"]}'
    shell:
        'sed '
            '-n {params.sed_script} '
            '{input} '
        '| shuf {params.genes} '
        '> {output} '

ruleorder:
    make_query_fasta > make_query_fasta_no_targets

def check_targets(wildcards):
    # non-focal species
    if wildcards.species != focal_species:
        return paths['reciprocal_genes'].format(**wildcards)
    if wildcards.analysis_type == paths['validation_dir']:
        return paths['validate_query_genes']
    # targets set
    if 'query_genes' in paths:
        return paths['query_genes']
    # just copy faa for focal species, in next rule
    raise ValueError

rule make_query_fasta:
    '''Produce fasta for query genes or reciprocal matches.'''
    input:
        target=check_targets,
        faa=paths['faa'],

    output:
        paths['query_fasta']

    conda: 'envs/seqkit.yaml'

    shell:
        'seqkit grep '
            '--pattern-file {input.target} '
            '{input.faa} '
            '--out-file {output} '
            '2>/dev/null\n'

rule make_query_fasta_no_targets:
    """If no query genes are provided, just copy over fasta.

    Only for the focal species, others should throw an error
    """
    input:
        faa=expand(paths['faa'], species=focal_species)

    output:
        expand(paths['query_fasta'], species=focal_species, analysis_type='')

    shell:
        'cp {input.faa} {output}\n'  # no targets

def blast_input(wildcards):
    return {
            'query': paths['query_fasta'].format(species=wildcards.query, **wildcards),
            'db': paths['blast_db'].format(species=wildcards.database, **wildcards),
            }

rule blast_search:
    input: unpack(blast_input)
    output: paths['blast_scores']
    conda: 'envs/blast.yaml'
    shell:
        'blastp '
            '-query {input.query} '
            '-db {input.db} '
            '-outfmt "6 qacc sacc bitscore evalue" '
            '-max_target_seqs 1 '
            '2>/dev/null '
            '> {output} '

rule make_gene_targets:
    input:
        lambda wildcards: expand(paths['blast_scores'],
                query=focal_species,
                database=wildcards.species,
                allow_missing=True,
                )
    output:
        paths['reciprocal_genes']

    shell:
        'cut -f2 {input} > {output}'

def determine_bit_scores_input(wildcards):
    return {
            'self_match': expand(paths['blast_scores'],
                analysis_type=wildcards.analysis_type,
                query=focal_species,
                database=focal_species),
            'forward': expand(paths['blast_scores'],
                analysis_type=wildcards.analysis_type,
                query=focal_species,
                database=other_species),
            'reciprocal': expand(paths['blast_scores'],
                analysis_type=wildcards.analysis_type,
                query=other_species,
                database=focal_species),
        }

rule determine_bit_scores:
    """Search for reciprocal best matches.

    If the gene is present multiple times, take lowest evalue
    If a gene is present in forward but at high e value => 0
    If the e value passes but reciprocal gene doesn't match => N/A
    If evalue and reciprocal search pass, use forward bitscore
    """
    input: unpack(determine_bit_scores_input)

    params:
        species=','.join(species),
        e_val_threshold=config['blast_e_val']

    output:
        paths['bit_scores']

    script:
        'scripts/join_blast_results.py'

rule calculate_db_lengths:
    '''Determine lengths of each species' database with esl-seqstat.'''
    input: expand(paths['faa'], species=species)

    output: paths['db_lengths']

    conda: 'envs/hmmer.yaml'

    shell:
        'rm -f {output}\n'  # if it exists
        'for file in {input} ; do '
            "size=$(esl-seqstat --amino $file "
                # find line starting with Total and get digit
                "| sed -nE '/^Total # residues/s/[^[:digit:]]+//p' )\n"
            'echo -e $(basename $file .faa)"\\t"$size >> {output} \n'
        'done '

rule calculate_gene_lengths:
    '''Determine lengths of each gene in focal species with esl-seqstat.'''
    input: expand(paths['faa'], species=focal_species)

    output: paths['gene_lengths']

    conda: 'envs/hmmer.yaml'

    shell:
        'esl-seqstat '
            '--amino '
            '-a '
            '{input} '
        '| sed -nE '
        # for lines starting with = capture first word and digit
            "'/^=/s/^= ([^ ]+) +([[:digit:]]+).*/\\1\\t\\2/p' "
        '> {output} '

ruleorder:
    absense_validate > absense  # wildcard constraints for validate to run

rule absense:
    '''Run absense analysis on query genes.'''
    input:
        bit_scores=paths['bit_scores'],
        distances=paths['distances'],
        gene_lengths=paths['gene_lengths'],
        db_lengths=paths['db_lengths'],

    output:
        expand(paths['absense_dir'] + '/{files}',
                files=[
                    '99PI_high_prediction.tsv',
                    '99PI_lower_prediction.tsv',
                    'failure_probabilities.tsv',
                    'parameters.tsv',
                    'predicted_bitscores.tsv',
                    'run_info.txt',
                    ],
                 allow_missing=True,
              )  

    params:
        out_dir=lambda wildcards: paths['absense_dir'].format(**wildcards)

    conda: 'envs/absense.yaml'

    shell:
        'absense '
            '--distances {input.distances} '
            '--bitscores {input.bit_scores} '
            '--gene-lengths {input.gene_lengths} '
            '--db-lengths {input.db_lengths} '
            '--out-dir {params.out_dir} '

rule absense_validate:
    '''Run absense analysis on validation genes.'''
    input:
        bit_scores=paths['bit_scores'],
        distances=paths['distances'],
        gene_lengths=paths['gene_lengths'],
        db_lengths=paths['db_lengths'],

    output:
        expand(paths['absense_dir'] + '/{files}',
                files=[
                    'run_info.txt',
                    'validation.tsv',
                    ],
                allow_missing=True)
    wildcard_constraints:
        analysis_type=paths['validation_dir']

    params:
        out_dir=lambda wildcards: paths['absense_dir'].format(**wildcards)

    conda: 'envs/absense.yaml'

    shell:
        'absense '
            '--distances {input.distances} '
            '--bitscores {input.bit_scores} '
            '--gene-lengths {input.gene_lengths} '
            '--db-lengths {input.db_lengths} '
            '--out-dir {params.out_dir} '
            '--validate '
